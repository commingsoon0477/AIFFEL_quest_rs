{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2968871-79e7-4ee0-bd8a-4ad8ddc3702a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Q            A  label\n",
      "0           12시 땡!   하루가 또 가네요.      0\n",
      "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
      "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
      "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
      "4          PPL 심하네   눈살이 찌푸려지죠.      0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 0.8487 | Token Accuracy: 0.0105\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# 0. 라이브러리 설치\n",
    "# =============================\n",
    "!pip install transformers sentencepiece pandas torch --quiet\n",
    "\n",
    "# =============================\n",
    "# 1. 데이터셋 로드\n",
    "# =============================\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "data = pd.read_csv(\"~/work/transformer_chatbot/data/ChatbotData.csv\")\n",
    "print(data.head(), flush=True)\n",
    "\n",
    "questions = data['Q'].astype(str).tolist()\n",
    "answers   = data['A'].astype(str).tolist()\n",
    "\n",
    "# =============================\n",
    "# 2. 토크나이저 (SentencePiece 기반)\n",
    "# =============================\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\n",
    "    \"skt/kogpt2-base-v2\",\n",
    "    bos_token=\"<bos>\",\n",
    "    eos_token=\"<eos>\",\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    mask_token=\"<mask>\"\n",
    ")\n",
    "\n",
    "# =============================\n",
    "# 3. 데이터셋 클래스\n",
    "# =============================\n",
    "class ChatDataset(Dataset):\n",
    "    def __init__(self, questions, answers, tokenizer, max_len=64):\n",
    "        self.questions = questions\n",
    "        self.answers = answers\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        q = self.questions[idx]\n",
    "        a = self.answers[idx]\n",
    "\n",
    "        q_enc = self.tokenizer.encode_plus(\n",
    "            q, truncation=True, padding=\"max_length\",\n",
    "            max_length=self.max_len, return_tensors=\"pt\"\n",
    "        )\n",
    "        a_enc = self.tokenizer.encode_plus(\n",
    "            a, truncation=True, padding=\"max_length\",\n",
    "            max_length=self.max_len, return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": q_enc[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": q_enc[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": a_enc[\"input_ids\"].squeeze()\n",
    "        }\n",
    "\n",
    "train_dataset = ChatDataset(questions, answers, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# =============================\n",
    "# 4. 모델 정의\n",
    "# =============================\n",
    "from transformers import GPT2LMHeadModel\n",
    "from torch.optim import AdamW\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"skt/kogpt2-base-v2\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# =============================\n",
    "# 5. 학습 루프 (loss + token accuracy)\n",
    "# =============================\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "epochs = 3\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits  # [batch_size, seq_len, vocab_size]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # =============================\n",
    "        # Token-level accuracy 계산\n",
    "        # =============================\n",
    "        with torch.no_grad():\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            mask = labels.ne(tokenizer.pad_token_id)\n",
    "            correct = (preds == labels) & mask\n",
    "            total_correct += correct.sum().item()\n",
    "            total_tokens += mask.sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    avg_acc = total_correct / total_tokens\n",
    "    print(f\"Epoch {epoch+1} | Loss: {avg_loss:.4f} | Token Accuracy: {avg_acc:.4f}\", flush=True)\n",
    "\n",
    "# =============================\n",
    "# 6. 챗봇 테스트 함수\n",
    "# =============================\n",
    "def chat(model, tokenizer, text, max_len=50):\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(text + tokenizer.eos_token, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_len,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            repetition_penalty=1.2,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.95\n",
    "        )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# =============================\n",
    "# 7. 챗봇 테스트\n",
    "# =============================\n",
    "print(chat(model, tokenizer, \"안녕?\"))\n",
    "print(chat(model, tokenizer, \"오늘 기분 어때?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa8eed3-7dd2-4fa9-8d4c-fe811886ad03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

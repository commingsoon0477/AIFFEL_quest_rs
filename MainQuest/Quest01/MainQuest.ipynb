{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb535768-20f7-4e4d-a4bc-f7d075288408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QUEST 1] Baseline Transformer 대비 변경·차별 포인트(요약)\n",
      "- 인코더/디코더 완전 Transformer가 아닌, GPT 스타일 '디코더-온리(Decoder-only)' 언어모델로 단순화.\n",
      "- [개선] 질문 뒤에 전용 구분 토큰 <ANS>(SEP)를 도입하여 Q/ A 경계를 명확히 함.\n",
      "  · 학습: [BOS] Q [SEP] A [EOS]  (Q구간은 loss에서 무시, A만 예측)\n",
      "  · 추론: [BOS] Q [SEP] … (A만 생성) → Q 복창/에코링 감소\n",
      "- [개선] 디코딩 제어: temperature + nucleus(top_p) + repetition_penalty + no_repeat_ngram + min_length\n",
      "  → 의미불명 반복과 횡설수설 억제, “말이 되는” 답변 유도\n",
      "- [개선] label smoothing(0.1) + cosine 스케줄(+warmup) → 안정적 수렴/일반화 향상.\n",
      "  ※ 검증은 smoothing=0.0으로 순수 NLL을 측정해 ppl 왜곡을 방지.\n",
      "- SentencePiece BPE로 한국어 토큰화, 실제 vocab size 동기화.\n",
      "- 속도 최적화: 작은 모델(d_model=256, n_layers=6, n_heads=8), AMP, 자동 배치, 샘플/길이 제한.\n",
      "\n",
      "[QUEST 2] 전처리 점검 예시:\n",
      "- 샘플 1\n",
      "  Q: 12시 땡!\n",
      "  A: 하루가 또 가네요.\n",
      "  seq(ids)[:24]: [1, 5566, 6801, 3207, 6907, 8000, 4489, 211, 5936, 6760, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  labels[:24]:   [-100, -100, -100, -100, -100, -100, 211, 5936, 6760, 2, 0, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "  유효타겟존재?: True\n",
      "- 샘플 2\n",
      "  Q: 1지망 학교 떨어졌어\n",
      "  A: 위로해 드립니다.\n",
      "  seq(ids)[:24]: [1, 344, 6768, 7074, 1006, 2458, 8000, 1619, 6422, 6760, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  labels[:24]:   [-100, -100, -100, -100, -100, -100, -100, 6422, 6760, 2, 0, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "  유효타겟존재?: True\n",
      "Train samples: 11586 → usable: 11586\n",
      "Valid samples: 237 → usable: 237\n",
      "SPM vocab size (original): 8000, +SEP → 8001\n",
      "[경고] CUDA OOM 감지 → CPU로 자동 전환합니다.\n",
      "[QUEST 3] 입력 블록/마스크 점검:\n",
      " - logits shape: (2, 16, 8001)\n",
      " - token/position embedding 합성 및 Pre-LN/Residual 적용 확인(코드 상 구현)\n",
      " - causal mask 상삼각 차단(bool) 적용(코드 상 구현)\n",
      " - PAD key_padding_mask 적용(코드 상 구현)\n",
      "\n",
      "\n",
      "=== [QUEST 4] Model Summary ===\n",
      "GPTSmall(\n",
      "  (tok_emb): Embedding(8001, 256)\n",
      "  (pos_emb): Embedding(64, 256)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      "  (blocks): ModuleList(\n",
      "    (0-5): 6 x GPTBlock(\n",
      "      (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (1): GELU()\n",
      "        (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (ln_f): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  (lm_head): Linear(in_features=256, out_features=8001, bias=False)\n",
      ")\n",
      "Total params: 6,803,712\n",
      "Trainable params: 6,803,712\n",
      "===============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ✅ 빠르게 동작하도록 조정한 GPT-style Chatbot training script (PyTorch / fallback bigram)\n",
    "# (이미지의 [QUEST 평가기준]을 코드에 반영하여 1~5 항목별 주석 및 점검 출력을 추가)\n",
    "# -------------------------------------------------------------------------------------------\n",
    "# [QUEST 1] Transformer와 비교해 변경/차별된 부분을 텍스트 블록으로 서술\n",
    "# [QUEST 2] 입력 형태(디코더-온리 LM)에 맞춘 전처리 수행 및 검증 출력\n",
    "# [QUEST 3] GPT 논문(https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)\n",
    "#           기준의 입력 블록(token + position, causal mask, pre-LN, residual) 구성 및 점검\n",
    "# [QUEST 4] GPT 모델 정상 구성 확인(model summary, 학습 루프/fit 로그)\n",
    "# [QUEST 5] 주어진 입력에 따른 생성 출력 확인(generation 데모)\n",
    "# -------------------------------------------------------------------------------------------\n",
    "\n",
    "# [PLUS] GPU 메모리 파편화 완화(파이토치 import '전'에 설정)\n",
    "import os\n",
    "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True\")\n",
    "\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sentencepiece as spm\n",
    "\n",
    "# ========================= [QUEST 1] 변경/차별 사항 서술 블록 =========================\n",
    "CHANGELOG_QUEST1 = \"\"\"\n",
    "[QUEST 1] Baseline Transformer 대비 변경·차별 포인트(요약)\n",
    "- 인코더/디코더 완전 Transformer가 아닌, GPT 스타일 '디코더-온리(Decoder-only)' 언어모델로 단순화.\n",
    "- [개선] 질문 뒤에 전용 구분 토큰 <ANS>(SEP)를 도입하여 Q/ A 경계를 명확히 함.\n",
    "  · 학습: [BOS] Q [SEP] A [EOS]  (Q구간은 loss에서 무시, A만 예측)\n",
    "  · 추론: [BOS] Q [SEP] … (A만 생성) → Q 복창/에코링 감소\n",
    "- [개선] 디코딩 제어: temperature + nucleus(top_p) + repetition_penalty + no_repeat_ngram + min_length\n",
    "  → 의미불명 반복과 횡설수설 억제, “말이 되는” 답변 유도\n",
    "- [개선] label smoothing(0.1) + cosine 스케줄(+warmup) → 안정적 수렴/일반화 향상.\n",
    "  ※ 검증은 smoothing=0.0으로 순수 NLL을 측정해 ppl 왜곡을 방지.\n",
    "- SentencePiece BPE로 한국어 토큰화, 실제 vocab size 동기화.\n",
    "- 속도 최적화: 작은 모델(d_model=256, n_layers=6, n_heads=8), AMP, 자동 배치, 샘플/길이 제한.\n",
    "\"\"\"\n",
    "print(CHANGELOG_QUEST1.strip())\n",
    "# ================================================================================\n",
    "\n",
    "# ---------------------------- Try import PyTorch ----------------------------\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    from torch.utils.data import Dataset, DataLoader\n",
    "    from torch.optim import AdamW\n",
    "    from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "    HAS_TORCH = True\n",
    "except Exception:\n",
    "    HAS_TORCH = False\n",
    "    torch = None\n",
    "    nn = object\n",
    "    F = None\n",
    "    Dataset = object\n",
    "    DataLoader = None\n",
    "    AdamW = None\n",
    "    CosineAnnealingLR = None\n",
    "    print(\"[경고] PyTorch 미탑재 환경. Numpy Bigram LM 모드로 실행\")\n",
    "\n",
    "# ---------------------------- [PLUS] GPU 캐시 정리 유틸 ----------------------------\n",
    "def _free_cuda():\n",
    "    try:\n",
    "        import gc\n",
    "        gc.collect()\n",
    "        if HAS_TORCH and torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            if hasattr(torch.cuda, \"ipc_collect\"):\n",
    "                torch.cuda.ipc_collect()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# ---------------------------- Config ----------------------------\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "MAX_SAMPLES = 100000\n",
    "MAX_LEN = 64\n",
    "VOCAB_SIZE_FOR_SPM = 8000\n",
    "\n",
    "DEFAULT_D_MODEL = 256\n",
    "DEFAULT_N_LAYERS = 6\n",
    "DEFAULT_N_HEADS = 8\n",
    "\n",
    "SPM_PREFIX = os.path.expanduser(\"~/work/transformer_chatbot/data/spm_kr\")\n",
    "DATA_DIR = os.path.expanduser(\"~/work/transformer_chatbot/data\")\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "DATA_PATH = os.path.join(DATA_DIR, \"ChatbotData.csv\")\n",
    "\n",
    "# (NEW) label smoothing 설정을 학습/검증으로 분리\n",
    "TRAIN_SMOOTH = 0.10\n",
    "EVAL_SMOOTH  = 0.00   # ← ppl 산출 시엔 smoothing=0.0로 순수 NLL 측정\n",
    "\n",
    "# ---------------------------- [PLUS] 생성 하이퍼(자연스러움·반복 억제) ----------------------------\n",
    "GEN_TEMPERATURE = 0.7      # 0.8→0.7 : 엉뚱 샘플 감소\n",
    "GEN_TOP_P = 0.88           # 0.90→0.88\n",
    "GEN_TOP_K = 60             # tail 컷 보완\n",
    "GEN_REP_PENALTY = 1.22     # 반복 억제 (1.15~1.25 권장)\n",
    "GEN_NGRAM_NO_REPEAT = 3    # “세요.세요”류 연속 억제\n",
    "GEN_MIN_NEW_TOKENS = 12    # 빈/짧은 답변 방지\n",
    "BAN_UNK_AT_GEN = True      # UNK 금지\n",
    "ECHO_PENALTY = 0.6         # 질문 토큰 에코 억제(로짓 감소)\n",
    "\n",
    "# ---------------------------- Device ----------------------------\n",
    "if HAS_TORCH:\n",
    "    torch.manual_seed(SEED)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    USE_CUDA = device.type == \"cuda\"\n",
    "    if USE_CUDA:\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "else:\n",
    "    device = None\n",
    "    USE_CUDA = False\n",
    "\n",
    "# ---------------------------- Download data if needed ----------------------------\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    print(\"Downloading ChatbotData.csv ...\")\n",
    "    urllib.request.urlretrieve(\n",
    "        \"https://github.com/songys/Chatbot_data/raw/master/ChatbotData.csv\",\n",
    "        DATA_PATH\n",
    "    )\n",
    "\n",
    "# ---------------------------- (NEW) 간단 텍스트 정규화 ----------------------------\n",
    "_url_pat = re.compile(r\"https?://\\S+\")\n",
    "_space_pat = re.compile(r\"\\s+\")\n",
    "_lol_pat = re.compile(r\"[ㅋㅎ]{2,}\")\n",
    "_rep_punct_pat = re.compile(r\"([!?~.])\\1{1,}\")\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    s = str(s)\n",
    "    s = _url_pat.sub(\"\", s)                 # URL 제거\n",
    "    s = _lol_pat.sub(\"ㅎㅎ\", s)              # 과도한 ㅋ/ㅎ 축약\n",
    "    s = _rep_punct_pat.sub(r\"\\1\\1\", s)      # 과잉 반복 기호 축약\n",
    "    s = s.replace(\"\\u200b\", \"\")             # zero-width 삭제\n",
    "    s = _space_pat.sub(\" \", s).strip()      # 공백 정리\n",
    "    return s\n",
    "\n",
    "# ---------------------------- Load data & build corpus ----------------------------\n",
    "raw = pd.read_csv(DATA_PATH).dropna(subset=[\"Q\", \"A\"])[:MAX_SAMPLES]\n",
    "raw[\"Q\"] = raw[\"Q\"].astype(str).map(normalize_text)\n",
    "raw[\"A\"] = raw[\"A\"].astype(str).map(normalize_text)\n",
    "\n",
    "corpus_path = os.path.join(DATA_DIR, \"corpus.txt\")\n",
    "with open(corpus_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for q, a in zip(raw[\"Q\"], raw[\"A\"]):\n",
    "        if q and a:\n",
    "            f.write(q + \"\\n\")\n",
    "            f.write(a + \"\\n\")\n",
    "\n",
    "# ---------------------------- Train SentencePiece (if needed) ----------------------------\n",
    "if not (os.path.exists(SPM_PREFIX + \".model\") and os.path.exists(SPM_PREFIX + \".vocab\")):\n",
    "    spm.SentencePieceTrainer.Train(\n",
    "        input=corpus_path,\n",
    "        model_prefix=SPM_PREFIX,\n",
    "        vocab_size=VOCAB_SIZE_FOR_SPM,\n",
    "        character_coverage=1.0,\n",
    "        model_type=\"bpe\",\n",
    "        bos_id=1,   # BOS\n",
    "        eos_id=2,   # EOS\n",
    "        pad_id=0,   # PAD\n",
    "        unk_id=3    # UNK\n",
    "    )\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(SPM_PREFIX + \".model\")\n",
    "SPM_VOCAB_SIZE = sp.GetPieceSize()\n",
    "PAD_ID, BOS_ID, EOS_ID, UNK_ID = 0, 1, 2, 3\n",
    "\n",
    "# === (NEW) 전용 구분 토큰: 모델 임베딩에서만 추가 ===\n",
    "SEP_ID = SPM_VOCAB_SIZE                 # <ANS> 토큰 (SentencePiece 바깥에서 새 id 할당)\n",
    "VOCAB_PLUS = SPM_VOCAB_SIZE + 1         # 임베딩/LM헤드는 +1 크기\n",
    "\n",
    "def _clip_and_pad(ids, max_len):\n",
    "    if len(ids) < max_len:\n",
    "        return ids + [PAD_ID] * (max_len - len(ids))\n",
    "    return ids[:max_len]\n",
    "\n",
    "# ========================= [QUEST 2] 전처리(디코더-온리용) 및 점검 =========================\n",
    "def encode_pair(q_text, a_text, max_len=MAX_LEN):\n",
    "    \"\"\"\n",
    "    [QUEST 2] Decoder-only LM 학습을 위한 한 줄 시퀀스 구성:\n",
    "    seq = [BOS] + Q + [SEP] + A + [EOS]\n",
    "    labels: [SEP] 이전은 -100(무시), A 구간만 next-token 예측 대상으로 남김.\n",
    "    PAD/EOS 위치는 -100 처리.\n",
    "    \"\"\"\n",
    "    q_ids = sp.EncodeAsIds(normalize_text(q_text))\n",
    "    a_ids = sp.EncodeAsIds(normalize_text(a_text))\n",
    "    seq = [BOS_ID] + q_ids + [SEP_ID] + a_ids + [EOS_ID]\n",
    "    seq = _clip_and_pad(seq, max_len)\n",
    "    # SEP까지는 전부 무시\n",
    "    try:\n",
    "        sep_pos = seq.index(SEP_ID) + 1  # SEP 포함 지점까지 무시\n",
    "    except ValueError:\n",
    "        sep_pos = min(1 + len(q_ids), max_len)\n",
    "    labels = seq[1:] + [PAD_ID]\n",
    "    labels = labels[:max_len]\n",
    "    for i in range(sep_pos):\n",
    "        labels[i] = -100\n",
    "    for i in range(max_len):\n",
    "        if seq[i] == PAD_ID:\n",
    "            labels[i] = -100\n",
    "    labels[-1] = -100\n",
    "    has_target = any(l >= 0 for l in labels)\n",
    "    return seq, labels, has_target\n",
    "\n",
    "def preview_preprocessing(df, n=2):\n",
    "    \"\"\"[QUEST 2] 전처리 결과 간단 점검 출력\"\"\"\n",
    "    print(\"\\n[QUEST 2] 전처리 점검 예시:\")\n",
    "    for i, (q, a) in enumerate(zip(df[\"Q\"].astype(str), df[\"A\"].astype(str))):\n",
    "        if i >= n: break\n",
    "        seq, labels, ok = encode_pair(q, a)\n",
    "        print(f\"- 샘플 {i+1}\")\n",
    "        print(\"  Q:\", q)\n",
    "        print(\"  A:\", a)\n",
    "        print(\"  seq(ids)[:24]:\", seq[:24])\n",
    "        print(\"  labels[:24]:  \", labels[:24])\n",
    "        print(\"  유효타겟존재?:\", ok)\n",
    "preview_preprocessing(raw)\n",
    "\n",
    "# ---------------------------- Dataset ----------------------------\n",
    "class ChatbotQADataset(Dataset if HAS_TORCH else object):\n",
    "    def __init__(self, df, max_len=MAX_LEN):\n",
    "        qs = df[\"Q\"].astype(str).tolist()\n",
    "        as_ = df[\"A\"].astype(str).tolist()\n",
    "        self.samples = []\n",
    "        for q, a in zip(qs, as_):\n",
    "            seq, labels, ok = encode_pair(q, a, max_len)\n",
    "            if ok:\n",
    "                self.samples.append((seq, labels))\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_ids, y_ids = self.samples[idx]\n",
    "        if HAS_TORCH:\n",
    "            return torch.tensor(x_ids, dtype=torch.long), torch.tensor(y_ids, dtype=torch.long)\n",
    "        else:\n",
    "            return np.array(x_ids, dtype=np.int64), np.array(y_ids, dtype=np.int64)\n",
    "\n",
    "# ---------------------------- Model (GPT-style, Pre-LN) ----------------------------\n",
    "if HAS_TORCH:\n",
    "    class GELU(nn.Module):\n",
    "        def forward(self, x):\n",
    "            return F.gelu(x)\n",
    "\n",
    "    class GPTBlock(nn.Module):\n",
    "        \"\"\"\n",
    "        [QUEST 3] GPT 논문 입력 블록 규격의 핵심:\n",
    "        - Pre-LN: 각 sublayer 앞에 LayerNorm\n",
    "        - Masked self-attention (causal)\n",
    "        - MLP(FFN) + Residual 연결\n",
    "        참고: Radford et al., \"Improving Language Understanding by Generative Pre-Training\" (GPT-1)\n",
    "              https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf\n",
    "        \"\"\"\n",
    "        def __init__(self, d_model, n_heads, mlp_ratio=4.0, dropout=0.1):\n",
    "            super().__init__()\n",
    "            self.ln1 = nn.LayerNorm(d_model)\n",
    "            self.attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n",
    "            self.ln2 = nn.LayerNorm(d_model)\n",
    "            self.mlp = nn.Sequential(\n",
    "                nn.Linear(d_model, int(d_model * mlp_ratio)),\n",
    "                GELU(),\n",
    "                nn.Linear(int(d_model * mlp_ratio), d_model),\n",
    "                nn.Dropout(dropout)\n",
    "            )\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        def forward(self, x, attn_mask=None, key_padding_mask=None):\n",
    "            h = self.ln1(x)\n",
    "            attn_out, _ = self.attn(\n",
    "                h, h, h,\n",
    "                attn_mask=attn_mask,\n",
    "                key_padding_mask=key_padding_mask,\n",
    "                need_weights=False\n",
    "            )\n",
    "            x = x + self.dropout(attn_out)\n",
    "            h2 = self.ln2(x)\n",
    "            x = x + self.mlp(h2)\n",
    "            return x\n",
    "\n",
    "    class GPTSmall(nn.Module):\n",
    "        \"\"\"\n",
    "        [QUEST 3] 토큰 임베딩 + 절대 위치 임베딩(learned) 합성 → 드롭아웃 → 블록 스택 → 최종 LN → LM 헤드\n",
    "        causal mask를 forward에서 생성해 상삼각을 -inf로 차단.\n",
    "        \"\"\"\n",
    "        def __init__(self, vocab_size, d_model=DEFAULT_D_MODEL, n_layers=DEFAULT_N_LAYERS,\n",
    "                     n_heads=DEFAULT_N_HEADS, max_len=MAX_LEN, dropout=0.1, tie_weights=True):\n",
    "            super().__init__()\n",
    "            self.max_len = max_len\n",
    "            self.tok_emb = nn.Embedding(vocab_size, d_model)  # vocab_size = VOCAB_PLUS\n",
    "            self.pos_emb = nn.Embedding(max_len, d_model)      # 절대 위치 임베딩 (learned)\n",
    "            self.drop = nn.Dropout(dropout)\n",
    "            self.blocks = nn.ModuleList([GPTBlock(d_model, n_heads, dropout=dropout) for _ in range(n_layers)])\n",
    "            self.ln_f = nn.LayerNorm(d_model)\n",
    "            self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "            if tie_weights:\n",
    "                self.lm_head.weight = self.tok_emb.weight\n",
    "\n",
    "        def forward(self, idx, labels=None, label_smoothing=0.1):\n",
    "            B, T = idx.size()\n",
    "            assert T <= self.max_len, \"시퀀스 길이가 max_len을 초과했습니다.\"\n",
    "            pos = torch.arange(T, device=idx.device).unsqueeze(0)  # [1, T]\n",
    "            x = self.tok_emb(idx) + self.pos_emb(pos)              # token + position\n",
    "            x = self.drop(x)\n",
    "\n",
    "            # [PLUS] causal mask: 상삼각 True=차단 (bool 마스크로 경고 제거)\n",
    "            attn_mask = torch.triu(torch.ones((T, T), device=idx.device, dtype=torch.bool), diagonal=1)\n",
    "            key_padding_mask = (idx == PAD_ID)  # bool\n",
    "\n",
    "            for blk in self.blocks:\n",
    "                x = blk(x, attn_mask=attn_mask, key_padding_mask=key_padding_mask)\n",
    "            x = self.ln_f(x)\n",
    "            logits = self.lm_head(x)\n",
    "            loss = None\n",
    "            if labels is not None:\n",
    "                loss = F.cross_entropy(\n",
    "                    logits.view(-1, logits.size(-1)),\n",
    "                    labels.view(-1),\n",
    "                    ignore_index=-100,\n",
    "                    label_smoothing=label_smoothing\n",
    "                )\n",
    "            return logits, loss\n",
    "\n",
    "    def model_summary(model):\n",
    "        # [QUEST 4] model summary 출력\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        print(\"\\n=== [QUEST 4] Model Summary ===\")\n",
    "        print(model)\n",
    "        print(f\"Total params: {total_params:,}\")\n",
    "        print(f\"Trainable params: {trainable_params:,}\")\n",
    "        print(\"===============================\\n\")\n",
    "\n",
    "# ---------------------------- Decode helper (SEP 제외) ----------------------------\n",
    "# [PLUS] 후처리 강화: 짧은 조각 반복 접기, 공백/구두점 정리, 빈 문자열 방지\n",
    "def _clean_text(s: str) -> str:\n",
    "    s = s.replace(\"\\u200b\", \"\")\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "\n",
    "    # 구두점 과반복 줄이기 (???, !!! → ??, !!)\n",
    "    s = re.sub(r\"([!?~.])\\1{2,}\", r\"\\1\\1\", s)\n",
    "\n",
    "    # 2~6글자 짧은 조각 반복을 1회로 접기 (예: \"하세요하세요\" → \"하세요\", \"세요.세요\" → \"세요.\")\n",
    "    s = re.sub(r'(\\S{2,6})(\\s?\\1){1,}', r'\\1', s)\n",
    "\n",
    "    # 공백·구두점 간격 정리\n",
    "    s = re.sub(r\"\\s+([,.!?])\", r\"\\1\", s).strip()\n",
    "\n",
    "    # 너무 길면 2문장까지만\n",
    "    parts = re.split(r'(?<=[.!?])\\s+', s)\n",
    "    if len(parts) > 2:\n",
    "        s = \" \".join(parts[:2]).strip()\n",
    "    return s\n",
    "\n",
    "def decode_model_output(ids):\n",
    "    \"\"\"\n",
    "    SentencePiece에 없는 SEP_ID 등을 제거하고, 가능하면 [SEP] 이후(A만) 디코드.\n",
    "    [PLUS] UNK/PAD 제거 및 후처리 적용\n",
    "    \"\"\"\n",
    "    if SEP_ID in ids:\n",
    "        start = ids.index(SEP_ID) + 1\n",
    "        tail = ids[start:]\n",
    "    else:\n",
    "        tail = ids\n",
    "    filtered = [t for t in tail if (0 <= t < SPM_VOCAB_SIZE) and (t not in (BOS_ID, PAD_ID, EOS_ID, UNK_ID))]\n",
    "    if len(filtered) == 0:\n",
    "        return \"\"\n",
    "    out = sp.DecodeIds(filtered)\n",
    "    out = _clean_text(out)\n",
    "    return out\n",
    "\n",
    "# ---------------------------- Generate & chatbot test ----------------------------\n",
    "if HAS_TORCH:\n",
    "    @torch.no_grad()\n",
    "    def _violates_no_repeat(out_ids, next_id, n=3):\n",
    "        if n <= 1 or len(out_ids) < n-1: return False\n",
    "        gram = out_ids[-(n-1):] + [next_id]\n",
    "        for i in range(len(out_ids) - (n-1)):\n",
    "            if out_ids[i:i+n] == gram:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    # [PLUS] 시작 토큰이 구두점/이상 토큰이면 재샘플\n",
    "    def _is_bad_start(tok_id):\n",
    "        piece = sp.IdToPiece(tok_id) if 0 <= tok_id < SPM_VOCAB_SIZE else \"\"\n",
    "        return (piece in {\".\", \",\", \"!\", \"?\", \"…\"} or piece.strip()==\"\")\n",
    "\n",
    "    def _apply_top_k(logits, top_k):\n",
    "        if top_k and top_k > 0:\n",
    "            kth = torch.topk(logits, k=min(top_k, logits.size(-1))).values[:, -1]\n",
    "            logits[logits < kth] = -float(\"inf\")\n",
    "\n",
    "    def _apply_top_p(logits, top_p):\n",
    "        if top_p and 0 < top_p < 1.0:\n",
    "            sorted_logits, sorted_idx = torch.sort(logits, descending=True)\n",
    "            probs_sorted = F.softmax(sorted_logits, dim=-1)\n",
    "            cumprobs = torch.cumsum(probs_sorted, dim=-1)\n",
    "            cutoff = (cumprobs > top_p).float().argmax(dim=-1)\n",
    "            for b in range(logits.size(0)):\n",
    "                logits[b, sorted_idx[b, cutoff[b]+1:]] = -float(\"inf\")\n",
    "\n",
    "    @torch.inference_mode()  # [PLUS] 메모리 절감\n",
    "    def generate(\n",
    "        model, prompt, max_new_tokens=64,\n",
    "        temperature=GEN_TEMPERATURE, top_p=GEN_TOP_P, repetition_penalty=GEN_REP_PENALTY,\n",
    "        no_repeat_ngram_size=GEN_NGRAM_NO_REPEAT, min_new_tokens=GEN_MIN_NEW_TOKENS, max_len=MAX_LEN,\n",
    "        top_k=GEN_TOP_K\n",
    "    ):\n",
    "        # [BOS] Q [SEP] 로 시작(답변만 생성)\n",
    "        q_ids = sp.EncodeAsIds(normalize_text(prompt))\n",
    "        ids = [BOS_ID] + q_ids + [SEP_ID]\n",
    "        out = torch.tensor(ids, dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "        banned = {PAD_ID, BOS_ID, SEP_ID}\n",
    "        if BAN_UNK_AT_GEN:\n",
    "            banned.add(UNK_ID)\n",
    "\n",
    "        for step in range(max_new_tokens):\n",
    "            logits, _ = model(out)\n",
    "            logits = logits[:, -1, :]\n",
    "\n",
    "            # 특수 토큰 금지\n",
    "            for b in banned:\n",
    "                logits[:, b] = -float(\"inf\")\n",
    "\n",
    "            # [PLUS] 질문 에코 억제: 질문 토큰의 로짓 감소\n",
    "            if ECHO_PENALTY > 0:\n",
    "                for tok in set(q_ids):\n",
    "                    if 0 <= tok < logits.size(-1):\n",
    "                        logits[0, tok] -= ECHO_PENALTY\n",
    "\n",
    "            # repetition penalty\n",
    "            if repetition_penalty and repetition_penalty != 1.0:\n",
    "                for tok in set(out[0].tolist()):\n",
    "                    logits[0, tok] /= repetition_penalty\n",
    "\n",
    "            # temperature\n",
    "            if temperature and temperature > 0:\n",
    "                logits = logits / temperature\n",
    "\n",
    "            # top-k / top-p\n",
    "            _apply_top_k(logits, top_k)\n",
    "            _apply_top_p(logits, top_p)\n",
    "\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_id = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "            # [PLUS] 시작 토큰이 이상하면 대안 선택\n",
    "            if step == 0 and _is_bad_start(next_id):\n",
    "                for alt_id in torch.argsort(probs[0], descending=True).tolist():\n",
    "                    if alt_id in banned or alt_id == EOS_ID: continue\n",
    "                    if not _is_bad_start(alt_id):\n",
    "                        next_id = alt_id\n",
    "                        break\n",
    "\n",
    "            # no-repeat n-gram & 최소 길이 전 EOS 금지\n",
    "            out_list = out[0].tolist()\n",
    "            if no_repeat_ngram_size and _violates_no_repeat(out_list, next_id, n=no_repeat_ngram_size):\n",
    "                for alt_id in torch.argsort(probs[0], descending=True).tolist():\n",
    "                    if alt_id == next_id: \n",
    "                        continue\n",
    "                    if alt_id in banned or alt_id == EOS_ID:\n",
    "                        continue\n",
    "                    if not _violates_no_repeat(out_list, alt_id, n=no_repeat_ngram_size):\n",
    "                        next_id = alt_id\n",
    "                        break\n",
    "            if (next_id == EOS_ID) and (step < min_new_tokens):\n",
    "                for alt_id in torch.argsort(probs[0], descending=True).tolist():\n",
    "                    if alt_id not in banned and alt_id != EOS_ID:\n",
    "                        next_id = alt_id\n",
    "                        break\n",
    "\n",
    "            out = torch.cat([out, torch.tensor([[next_id]], device=device)], dim=1)\n",
    "            if next_id == EOS_ID or out.size(1) >= max_len:\n",
    "                break\n",
    "\n",
    "        text = decode_model_output(out[0].tolist())\n",
    "\n",
    "        # [PLUS] 빈 문자열 방어: 최소 보장 길이 없으면 안전한 기본 답변\n",
    "        if not text or len(text) < 4:\n",
    "            text = \"음... 조금 더 자세히 말해줄래?\"\n",
    "        return text\n",
    "\n",
    "    def chatbot_test(model):\n",
    "        # [QUEST 5] 생성 결과 데모 출력\n",
    "        prompts = [\n",
    "            \"안녕!\",\n",
    "            \"오늘 날씨 어때?\",\n",
    "            \"내일 힘이 날 말 한마디 해줘\",\n",
    "            \"주말에 뭐하면 좋아?\",\n",
    "            \"파이토치 설치가 안돼. 어떻게 해야 해?\"\n",
    "        ]\n",
    "        print(\"\\n[QUEST 5] Chatbot Test 시작\")\n",
    "        for p in prompts:\n",
    "            ans = generate(model, p, max_new_tokens=64)\n",
    "            print(f\"[Q] {p}\\n[A] {ans}\\n\")\n",
    "        print(\"[QUEST 5] Chatbot Test 완료\\n\")\n",
    "\n",
    "# ---------------------------- Data split / loaders ----------------------------\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, valid_df = train_test_split(raw, test_size=0.02, random_state=SEED)\n",
    "\n",
    "if HAS_TORCH:\n",
    "    train_ds = ChatbotQADataset(train_df)\n",
    "    valid_ds = ChatbotQADataset(valid_df)\n",
    "\n",
    "    BATCH_SIZE = 32 if USE_CUDA else 8\n",
    "    NUM_WORKERS = min(4, max(0, os.cpu_count() - 1))\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, pin_memory=USE_CUDA, num_workers=NUM_WORKERS)\n",
    "    valid_loader = DataLoader(valid_ds, batch_size=BATCH_SIZE, shuffle=False, pin_memory=USE_CUDA, num_workers=NUM_WORKERS)\n",
    "else:\n",
    "    # fallback bigram (학습 없이도 최소 생성 가능)\n",
    "    counts = np.zeros((VOCAB_PLUS, VOCAB_PLUS), dtype=np.float64)\n",
    "    for q, a in zip(raw[\"Q\"].astype(str), raw[\"A\"].astype(str)):\n",
    "        seq = [BOS_ID] + sp.EncodeAsIds(normalize_text(q)) + [SEP_ID] + sp.EncodeAsIds(normalize_text(a)) + [EOS_ID]\n",
    "        for x, y in zip(seq[:-1], seq[1:]):\n",
    "            if 0 <= x < VOCAB_PLUS and 0 <= y < VOCAB_PLUS:\n",
    "                counts[x, y] += 1\n",
    "    probs = counts + 1.0\n",
    "    probs /= probs.sum(axis=1, keepdims=True)\n",
    "\n",
    "# ---------------------------- [QUEST 3] 입력 블록/마스크 검증 유틸 ----------------------------\n",
    "if HAS_TORCH:\n",
    "    def verify_input_block(model):\n",
    "        print(\"[QUEST 3] 입력 블록/마스크 점검:\")\n",
    "        x = torch.randint(4, VOCAB_PLUS, (2, 16), device=device)  # [B=2, T=16]\n",
    "        x[:, -2:] = PAD_ID  # 일부 PAD 삽입\n",
    "        logits, loss = model(x, labels=torch.full_like(x, -100))\n",
    "        B, T = x.size()\n",
    "        assert logits.shape[:2] == (B, T), \"logits shape 불일치\"\n",
    "        print(\" - logits shape:\", tuple(logits.shape))\n",
    "        print(\" - token/position embedding 합성 및 Pre-LN/Residual 적용 확인(코드 상 구현)\")\n",
    "        print(\" - causal mask 상삼각 차단(bool) 적용(코드 상 구현)\")\n",
    "        print(\" - PAD key_padding_mask 적용(코드 상 구현)\\n\")\n",
    "\n",
    "# ---------------------------- Training loop with AMP + Cosine LR ----------------------------\n",
    "if HAS_TORCH:\n",
    "    print(f\"Train samples: {len(train_df)} → usable: {len(train_ds)}\")\n",
    "    print(f\"Valid samples: {len(valid_df)} → usable: {len(valid_ds)}\")\n",
    "    print(f\"SPM vocab size (original): {SPM_VOCAB_SIZE}, +SEP → {VOCAB_PLUS}\")\n",
    "\n",
    "    # [PLUS] 모델 생성 전 캐시 정리 + OOM 시 CPU 폴백\n",
    "    _free_cuda()\n",
    "    try:\n",
    "        model = GPTSmall(vocab_size=VOCAB_PLUS, d_model=DEFAULT_D_MODEL,\n",
    "                         n_layers=DEFAULT_N_LAYERS, n_heads=DEFAULT_N_HEADS).to(device)\n",
    "    except RuntimeError as e:\n",
    "        if \"out of memory\" in str(e).lower():\n",
    "            print(\"[경고] CUDA OOM 감지 → CPU로 자동 전환합니다.\")\n",
    "            _free_cuda()\n",
    "            device = torch.device(\"cpu\")\n",
    "            USE_CUDA = False\n",
    "            train_loader = DataLoader(train_ds, batch_size=8, shuffle=True, pin_memory=False, num_workers=0)\n",
    "            valid_loader = DataLoader(valid_ds, batch_size=8, shuffle=False, pin_memory=False, num_workers=0)\n",
    "            model = GPTSmall(vocab_size=VOCAB_PLUS, d_model=DEFAULT_D_MODEL,\n",
    "                             n_layers=DEFAULT_N_LAYERS, n_heads=DEFAULT_N_HEADS).to(device)\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "    # [QUEST 3] 입력 블록 구성/마스크 검증\n",
    "    verify_input_block(model)\n",
    "\n",
    "    # [QUEST 4] 모델 요약 출력\n",
    "    model_summary(model)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=3e-4, weight_decay=0.1)\n",
    "    EPOCHS = 80\n",
    "\n",
    "    # [PLUS] 최신 AMP API (경고 제거)\n",
    "    scaler = torch.amp.GradScaler('cuda', enabled=USE_CUDA)\n",
    "\n",
    "    # Warmup + Cosine\n",
    "    warmup_steps = max(100, len(train_loader))\n",
    "    total_steps = EPOCHS * max(1, len(train_loader))\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=total_steps - warmup_steps, eta_min=3e-5)\n",
    "\n",
    "    best_val = float(\"inf\")\n",
    "    patience, bad = 3, 0\n",
    "\n",
    "    global_step = 0\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for x, labels in train_loader:\n",
    "            x, labels = x.to(device), labels.to(device)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            if USE_CUDA:\n",
    "                with torch.amp.autocast('cuda'):\n",
    "                    _, loss = model(x, labels, label_smoothing=TRAIN_SMOOTH)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                _, loss = model(x, labels, label_smoothing=TRAIN_SMOOTH)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "\n",
    "            global_step += 1\n",
    "            if global_step > warmup_steps:\n",
    "                scheduler.step()\n",
    "            total_loss += float(loss.item()) * x.size(0)\n",
    "\n",
    "        train_loss = total_loss / max(1, len(train_ds))\n",
    "\n",
    "        model.eval()\n",
    "        total_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for x, labels in valid_loader:\n",
    "                x, labels = x.to(device), labels.to(device)\n",
    "                # ⚠️ 검증은 label_smoothing=0.0 로 순수 NLL 측정 → ppl 하락\n",
    "                if USE_CUDA:\n",
    "                    with torch.amp.autocast('cuda'):\n",
    "                        _, loss = model(x, labels, label_smoothing=EVAL_SMOOTH)\n",
    "                else:\n",
    "                    _, loss = model(x, labels, label_smoothing=EVAL_SMOOTH)\n",
    "                total_loss += float(loss.item()) * x.size(0)\n",
    "        val_loss = total_loss / max(1, len(valid_ds))\n",
    "        ppl = math.exp(min(20, val_loss))\n",
    "        print(f\"Epoch {epoch:02d} | train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | ppl={ppl:.2f}\")\n",
    "\n",
    "        if val_loss < best_val - 1e-3:\n",
    "            best_val = val_loss\n",
    "            bad = 0\n",
    "            torch.save(model.state_dict(), os.path.join(DATA_DIR, \"gptsmall_chatbot_best.pth\"))\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= patience:\n",
    "                print(f\"Early stopping triggered (patience={patience}).\")\n",
    "                break\n",
    "\n",
    "        if epoch % 2 == 0:\n",
    "            sample_q = random.choice(valid_df[\"Q\"].tolist())\n",
    "            print(\"[Sample Q]\", sample_q)\n",
    "            print(\"[Sample A]\", generate(model, sample_q, max_new_tokens=64))\n",
    "\n",
    "    # [QUEST 5] 최종 생성 데모\n",
    "    chatbot_test(model)\n",
    "\n",
    "    torch.save(model.state_dict(), os.path.join(DATA_DIR, \"gptsmall_chatbot_last.pth\"))\n",
    "    print(\"Model saved to:\", os.path.join(DATA_DIR, \"gptsmall_chatbot_last.pth\"))\n",
    "\n",
    "# ---------------------------- Fallback path note ----------------------------\n",
    "else:\n",
    "    print(\"\\n[주의] PyTorch 미탑재 환경에서는 Bigram 확률로만 문장을 샘플링할 수 있습니다. (데모 전용)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122ff190-e54f-4b25-b76e-ecd95de3ddd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
